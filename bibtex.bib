%% ---------------- Event Extraction ----------------
@inproceedings{Zhang_VAD_2017,
    title = {Improving Event Extraction via Multimodal Integration},
    author = {Zhang, Tongtao and Whitehead, Spencer and Zhang, Hanwang and Li, Hongzhi and Ellis, Joseph and Huang, Lifu and Liu, Wei and Ji, Heng and Chang, Shih-Fu},
    booktitle = {Proceedings of MM},
    year = {2017},
    url = {https://doi.org/10.1145/3123266.3123294},
    pages = {270â€“278},
    keywords = {Event Extraction, New Task, Visual Pattern Discovery, ACE2005, ERE},
    }
    @String(Zhang_VAD_2017="The first paper to do multimodal event extraction")

@inproceedings{li-etal-2020-cross,
    title = "Cross-media Structured Common Space for Multimedia Event Extraction",
    author = "Li, Manling  and
      Zareian, Alireza  and
      Zeng, Qi  and
      Whitehead, Spencer  and
      Lu, Di  and
      Ji, Heng  and
      Chang, Shih-Fu",
    booktitle = "Proceedings of ACL",
    year = "2020",
    url = "https://aclanthology.org/2020.acl-main.230",
    pages = "2557--2568",
    keywords = {Event Extraction, New Task, Vision-Language, WASE, AMR Graph, Situation Graph, GCN, M2E2, New Dataset},
    }
    @String(li-etal-2020-cross="The first paper to define a multimodal event extraction task")

@inproceedings{chen-etal-2021-joint-multimedia-event,
    title = "Joint Multimedia Event Extraction from Video and Article",
    author = "Chen, Brian  and
      Lin, Xudong  and
      Thomas, Christopher  and
      Li, Manling  and
      Yoshida, Shoya  and
      Chum, Lovish  and
      Ji, Heng  and
      Chang, Shih-Fu",
    booktitle = "EMNLP",
    year = "2021",
    url = "https://aclanthology.org/2021.findings-emnlp.8",
    pages = "74--88",
    keywords = {Event Extraction, New Task, Video-Language, VM2E2},
    }
    @String(chen-etal-2021-joint-multimedia-event="This paper proposes a new task of video multimodal event extraction")

@article{Tong_Wang_Cao_Xu_Li_Hou_Chua_2020,
    title = "Image Enhanced Event Detection in News Articles",
    author = "Tong, Meihan and Wang, Shuai and Cao, Yixin and Xu, Bin and Li, Juanzi and Hou, Lei and Chua, Tat-Seng",
    booktitle = "Proceedings of AAAI",
    year = "2020",
    url = "https://ojs.aaai.org/index.php/AAAI/article/view/6437",
    pages={9040-9047},
    keywords = {Event Extraction, Vision-Language, Alternating Dual Attention},
    }
    @String(Tong_Wang_Cao_Xu_Li_Hou_Chua_2020="This paper proposes a multimodal fusion method with alternating dual attention")


@article{Xiao_2022_Survey,
    TITLE = {A Survey of Data Representation for Multi-Modality Event Detection and Evolution},
    AUTHOR = {Xiao, Kejing and Qian, Zhaopeng and Qin, Biao},
    JOURNAL = {Applied Sciences},
    YEAR = {2022},
    URL = {https://www.mdpi.com/2076-3417/12/4/2204},
    keywords = {Survey},
    }


% ---------------- Relation Extraction ----------------
@inbook{Zheng_2021_MEGA,
    author = {Zheng, Changmeng and Feng, Junhao and Fu, Ze and Cai, Yi and Li, Qing and Wang, Tao},
    title = {Multimodal Relation Extraction with Efficient Graph Alignment},
    year = {2021},
    url = {https://doi.org/10.1145/3474085.3476968},
    booktitle = {Proceedings of MM},
    keywords = {Relation Extraction, Scene Graph, Syntax Parser Tree, BERT, MNER, New Dataset}
    }
    @String(Zheng_2021_MEGA="This paper proposes a Multimodal Neural Network with Efficient Graph Alignment (MEGA) method for relation extraction in social media posts")

@misc{Chen_2022_HVPNeT,
    title = {Good Visual Guidance Makes A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction},
    author = {Chen, Xiang and Zhang, Ningyu and Li, Lei and Yao, Yunzhi and Deng, Shumin and Tan, Chuanqi and Huang, Fei and Si, Luo and Chen, Huajun},
    journal = {CoRR},
    year = {2022},
    url = {https://arxiv.org/abs/2205.03521},
    keywords = {Relation Extraction, Prompt, ResNet50, BERT-base, Low-resource, Cross-task, MNER, MRE}
  }
  @String(Chen_2022_HVPNeT="This paper proposes visual prefix-guided fusion by concatenating object-level visual representation as the prefix of each self-attention layer in BERT")


% ---------------- Preprint ----------------
@article{Li_Clip_Event_2022,
  author    = {Manling Li and
               Ruochen Xu and
               Shuohang Wang and
               Luowei Zhou and
               Xudong Lin and
               Chenguang Zhu and
               Michael Zeng and
               Heng Ji and
               Shih{-}Fu Chang},
  title     = {CLIP-Event: Connecting Text and Images with Event Structures},
  journal   = {CoRR},
  year      = {2022},
  url       = {https://arxiv.org/abs/2201.05078},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2201-05078.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  keywords = {Event Extraction, Gaia, Faster R-CNN, VOANews}
  }
  @String(Li_Clip_Event_2022="This paper is inspired by CLIP using the Contrast Learning Framework, a method for connecting text and images using event structures")