# Multi-modal Information Extraction Literature 
This repository is maintained by [Tongtong Wu](http://wutong8023.site/) and [Jingqi Kang](https://#####). 

The automation script of this repo is powered by [Auto-Bibfile](https://github.com/wutong8023/Auto-Bibfile.git).

You can directly use our bibtex.bib in overleaf with this [link](https://www.overleaf.com/read/gszxbvbkprfs).

This page categorizes the literature by the **Continual Learning Approach**.

## Outline 
- [![](https://img.shields.io/badge/Hyperlink-blue)](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/approach\README.md#hyperlink)
- [![](https://img.shields.io/badge/Visual_Pattern_Discovery-1-blue)](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/approach\README.md#visual-pattern-discovery)
- [![](https://img.shields.io/badge/WASE-1-blue)](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/approach\README.md#wase)
- [![](https://img.shields.io/badge/AMR_Graph-1-blue)](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/approach\README.md#amr-graph)
- [![](https://img.shields.io/badge/Situation_Graph-1-blue)](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/approach\README.md#situation-graph)
- [![](https://img.shields.io/badge/GCN-1-blue)](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/approach\README.md#gcn)
- [![](https://img.shields.io/badge/Alternating_Dual_Attention-1-blue)](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/approach\README.md#alternating-dual-attention)
- [![](https://img.shields.io/badge/Prompt-1-blue)](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/approach\README.md#prompt)
## Hyperlink 
- [[Overview]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//README.md) -- [Homepage](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//README.md)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/./)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/./) -- [Summary](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/./)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/application)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/application) -- [Application](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/application)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/approach)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/approach) -- [Approach](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/approach)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/author)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/author) -- [Author](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/author)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/backbone_model)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/backbone_model) -- [Backbone Model](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/backbone_model)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/contribution)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/contribution) -- [Contribution](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/contribution)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/dataset)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/dataset) -- [Dataset](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/dataset)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/metrics)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/metrics) -- [Metrics](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/metrics)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/research_question)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/research_question) -- [Research Questions](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/research_question)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/setting)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/setting) -- [Setting](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/setting)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/supervision)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/supervision) -- [ Learning Paradigm](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/supervision)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/time)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/time) -- [Published Time](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/time)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/venue)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/venue) -- [Published Venue](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/venue)

## Visual Pattern Discovery

- [![](https://img.shields.io/badge/MM-2017-blue)](https://doi.org/10.1145/3123266.3123294) [**Improving Event Extraction via Multimodal Integration**](https://doi.org/10.1145/3123266.3123294) , <br> by *Zhang, Tongtao, Whitehead, Spencer, Zhang, Hanwang, Li, Hongzhi, Ellis, Joseph, Huang, Lifu, Liu, Wei, Ji, Heng and Chang, Shih-Fu* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L2-L10)<br> ```The first paper to do multimodal event extraction
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```Zhang_VAD_2017```
## WASE

- [![](https://img.shields.io/badge/ACL-2020-blue)](https://aclanthology.org/2020.acl-main.230) [**Cross-media Structured Common Space for Multimedia Event Extraction**](https://aclanthology.org/2020.acl-main.230) , <br> by *Li, Manling  and
Zareian, Alireza  and
Zeng, Qi  and
Whitehead, Spencer  and
Lu, Di  and
Ji, Heng  and
Chang, Shih-Fu* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L13-L27)<br> ```The first paper to define a multimodal event extraction task
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```li-etal-2020-cross```
## AMR Graph

- [![](https://img.shields.io/badge/ACL-2020-blue)](https://aclanthology.org/2020.acl-main.230) [**Cross-media Structured Common Space for Multimedia Event Extraction**](https://aclanthology.org/2020.acl-main.230) , <br> by *Li, Manling  and
Zareian, Alireza  and
Zeng, Qi  and
Whitehead, Spencer  and
Lu, Di  and
Ji, Heng  and
Chang, Shih-Fu* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L13-L27)<br> ```The first paper to define a multimodal event extraction task
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```li-etal-2020-cross```
## Situation Graph

- [![](https://img.shields.io/badge/ACL-2020-blue)](https://aclanthology.org/2020.acl-main.230) [**Cross-media Structured Common Space for Multimedia Event Extraction**](https://aclanthology.org/2020.acl-main.230) , <br> by *Li, Manling  and
Zareian, Alireza  and
Zeng, Qi  and
Whitehead, Spencer  and
Lu, Di  and
Ji, Heng  and
Chang, Shih-Fu* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L13-L27)<br> ```The first paper to define a multimodal event extraction task
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```li-etal-2020-cross```
## GCN

- [![](https://img.shields.io/badge/ACL-2020-blue)](https://aclanthology.org/2020.acl-main.230) [**Cross-media Structured Common Space for Multimedia Event Extraction**](https://aclanthology.org/2020.acl-main.230) , <br> by *Li, Manling  and
Zareian, Alireza  and
Zeng, Qi  and
Whitehead, Spencer  and
Lu, Di  and
Ji, Heng  and
Chang, Shih-Fu* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L13-L27)<br> ```The first paper to define a multimodal event extraction task
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```li-etal-2020-cross```
## Alternating Dual Attention

- [![](https://img.shields.io/badge/AAAI-2020-blue)](https://ojs.aaai.org/index.php/AAAI/article/view/6437) [**Image Enhanced Event Detection in News Articles**](https://ojs.aaai.org/index.php/AAAI/article/view/6437) , <br> by *Tong, Meihan, Wang, Shuai, Cao, Yixin, Xu, Bin, Li, Juanzi, Hou, Lei and Chua, Tat-Seng* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L48-L56)<br> ```This paper proposes a multimodal fusion method with alternating dual attention
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```Tong_Wang_Cao_Xu_Li_Hou_Chua_2020```
## Prompt

- [![](https://img.shields.io/badge/CoRR-2022-blue)](https://arxiv.org/abs/2205.03521) [**Good Visual Guidance Makes A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction**](https://arxiv.org/abs/2205.03521) , <br> by *Chen, Xiang, Zhang, Ningyu, Li, Lei, Yao, Yunzhi, Deng, Shumin, Tan, Chuanqi, Huang, Fei, Si, Luo and Chen, Huajun* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L81-L88)<br> ```This paper proposes visual prefix-guided fusion by concatenating object-level visual representation as the prefix of each self-attention layer in BERT
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```Chen_2022_HVPNeT```