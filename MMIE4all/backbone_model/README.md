# Multi-modal Information Extraction Literature 
This repository is maintained by [Tongtong Wu](http://wutong8023.site/) and [Jingqi Kang](https://#####). 

The automation script of this repo is powered by [Auto-Bibfile](https://github.com/wutong8023/Auto-Bibfile.git).

You can directly use our bibtex.bib in overleaf with this [link](https://www.overleaf.com/read/gszxbvbkprfs).

This page categorizes the literature by the **Backbone Model**.

## Outline 
- [![](https://img.shields.io/badge/Hyperlink-blue)](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/backbone_model\README.md#hyperlink)
- [![](https://img.shields.io/badge/BERT_base-1-blue)](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/backbone_model\README.md#bert-base)
- [![](https://img.shields.io/badge/Gaia-1-blue)](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/backbone_model\README.md#gaia)
- [![](https://img.shields.io/badge/ResNet50-1-blue)](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/backbone_model\README.md#resnet50)
- [![](https://img.shields.io/badge/Faster_R_CNN-1-blue)](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/backbone_model\README.md#faster-r-cnn)
## Hyperlink 
- [[Overview]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//README.md) -- [Homepage](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//README.md)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/./)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/./) -- [Summary](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/./)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/application)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/application) -- [Application](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/application)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/approach)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/approach) -- [Approach](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/approach)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/author)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/author) -- [Author](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/author)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/backbone_model)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/backbone_model) -- [Backbone Model](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/backbone_model)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/contribution)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/contribution) -- [Contribution](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/contribution)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/dataset)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/dataset) -- [Dataset](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/dataset)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/metrics)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/metrics) -- [Metrics](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/metrics)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/research_question)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/research_question) -- [Research Questions](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/research_question)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/setting)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/setting) -- [Setting](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/setting)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/supervision)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/supervision) -- [ Learning Paradigm](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/supervision)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/time)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/time) -- [Published Time](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/time)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/venue)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/venue) -- [Published Venue](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/venue)

## BERT-base

- [![](https://img.shields.io/badge/CoRR-2022-blue)](https://arxiv.org/abs/2205.03521) [**Good Visual Guidance Makes A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction**](https://arxiv.org/abs/2205.03521) , <br> by *Chen, Xiang, Zhang, Ningyu, Li, Lei, Yao, Yunzhi, Deng, Shumin, Tan, Chuanqi, Huang, Fei, Si, Luo and Chen, Huajun* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L81-L88)<br> ```This paper proposes visual prefix-guided fusion by concatenating object-level visual representation as the prefix of each self-attention layer in BERT
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```Chen_2022_HVPNeT```
## Gaia

- [![](https://img.shields.io/badge/CoRR-2022-blue)](https://arxiv.org/abs/2201.05078) [**CLIP-Event: Connecting Text and Images with Event Structures**](https://arxiv.org/abs/2201.05078) , <br> by *Manling Li and
Ruochen Xu and
Shuohang Wang and
Luowei Zhou and
Xudong Lin and
Chenguang Zhu and
Michael Zeng and
Heng Ji and
Shih{-}Fu Chang* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L93-L110)<br> ```This paper is inspired by CLIP using the Contrast Learning Framework, a method for connecting text and images using event structures
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```Li_Clip_Event_2022```
## ResNet50

- [![](https://img.shields.io/badge/CoRR-2022-blue)](https://arxiv.org/abs/2205.03521) [**Good Visual Guidance Makes A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction**](https://arxiv.org/abs/2205.03521) , <br> by *Chen, Xiang, Zhang, Ningyu, Li, Lei, Yao, Yunzhi, Deng, Shumin, Tan, Chuanqi, Huang, Fei, Si, Luo and Chen, Huajun* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L81-L88)<br> ```This paper proposes visual prefix-guided fusion by concatenating object-level visual representation as the prefix of each self-attention layer in BERT
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```Chen_2022_HVPNeT```
## Faster R-CNN

- [![](https://img.shields.io/badge/CoRR-2022-blue)](https://arxiv.org/abs/2201.05078) [**CLIP-Event: Connecting Text and Images with Event Structures**](https://arxiv.org/abs/2201.05078) , <br> by *Manling Li and
Ruochen Xu and
Shuohang Wang and
Luowei Zhou and
Xudong Lin and
Chenguang Zhu and
Michael Zeng and
Heng Ji and
Shih{-}Fu Chang* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L93-L110)<br> ```This paper is inspired by CLIP using the Contrast Learning Framework, a method for connecting text and images using event structures
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```Li_Clip_Event_2022```