# Multi-modal Information Extraction Literature 
This repository is maintained by [Tongtong Wu](http://wutong8023.site/) and [Jingqi Kang](https://#####). 

The automation script of this repo is powered by [Auto-Bibfile](https://github.com/wutong8023/Auto-Bibfile.git).

You can directly use our bibtex.bib in overleaf with this [link](https://www.overleaf.com/read/gszxbvbkprfs).

This page categorizes the literature by the **Backbone Model**.

## Outline 
- [![](https://img.shields.io/badge/Hyperlink-blue)](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/backbone_model\README.md#hyperlink)
- [![](https://img.shields.io/badge/BERT_base-1-blue)](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/backbone_model\README.md#bert-base)
- [![](https://img.shields.io/badge/Gaia-1-blue)](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/backbone_model\README.md#gaia)
- [![](https://img.shields.io/badge/Transformer-2-blue)](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/backbone_model\README.md#transformer)
- [![](https://img.shields.io/badge/ResNet50-3-blue)](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/backbone_model\README.md#resnet50)
- [![](https://img.shields.io/badge/Faster_R_CNN-1-blue)](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/backbone_model\README.md#faster-r-cnn)
- [![](https://img.shields.io/badge/Vision_Transformer(ViT)-2-blue)](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/backbone_model\README.md#vision-transformer(vit))
- [![](https://img.shields.io/badge/Adapter-1-blue)](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/backbone_model\README.md#adapter)
## Hyperlink 
- [[Overview]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//README.md) -- [Homepage](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//README.md)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/./)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/./) -- [Summary](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/./)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/application)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/application) -- [Application](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/application)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/approach)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/approach) -- [Approach](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/approach)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/author)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/author) -- [Author](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/author)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/backbone_model)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/backbone_model) -- [Backbone Model](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/backbone_model)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/contribution)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/contribution) -- [Contribution](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/contribution)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/dataset)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/dataset) -- [Dataset](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/dataset)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/metrics)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/metrics) -- [Metrics](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/metrics)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/research_question)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/research_question) -- [Research Questions](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/research_question)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/setting)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/setting) -- [Setting](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/setting)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/supervision)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/supervision) -- [ Learning Paradigm](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/supervision)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/time)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/time) -- [Published Time](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/time)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/venue)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/venue) -- [Published Venue](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/venue)

## BERT-base

- [![](https://img.shields.io/badge/CoRR-2022-blue)](https://arxiv.org/abs/2205.03521) [**Good Visual Guidance Makes A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction**](https://arxiv.org/abs/2205.03521) , <br> by *Chen, Xiang, Zhang, Ningyu, Li, Lei, Yao, Yunzhi, Deng, Shumin, Tan, Chuanqi, Huang, Fei, Si, Luo and Chen, Huajun* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L81-L88)<br> ```This paper proposes visual prefix-guided fusion by concatenating object-level visual representation as the prefix of each self-attention layer in BERT
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```Chen_2022_HVPNeT```
## Gaia

- [![](https://img.shields.io/badge/CoRR-2022-blue)](https://arxiv.org/abs/2201.05078) [**CLIP-Event: Connecting Text and Images with Event Structures**](https://arxiv.org/abs/2201.05078) , <br> by *Manling Li and
Ruochen Xu and
Shuohang Wang and
Luowei Zhou and
Xudong Lin and
Chenguang Zhu and
Michael Zeng and
Heng Ji and
Shih{-}Fu Chang* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L93-L110)<br> ```This paper is inspired by CLIP using the Contrast Learning Framework, a method for connecting text and images using event structures
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```Li_Clip_Event_2022```
## Transformer

- [![](https://img.shields.io/badge/CoRR-2021-blue)](https://arxiv.org/abs/2103.00020) [**Learning Transferable Visual Models From Natural Language Supervision**](https://arxiv.org/abs/2103.00020) , <br> by *Alec Radford and
Jong Wook Kim and
Chris Hallacy and
Aditya Ramesh and
Gabriel Goh and
Sandhini Agarwal and
Girish Sastry and
Amanda Askell and
Pamela Mishkin and
Jack Clark and
Gretchen Krueger and
Ilya Sutskever* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L114-L134)<br> ```This paper proposes to learn a multimodal embedding space by jointly training an image encoder and a text encoder
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```CLIP_2021```
- [![](https://img.shields.io/badge/CoRR-2021-blue)](https://arxiv.org/abs/2112.04478) [**Prompting Visual-Language Models for Efficient Video Understanding**](https://arxiv.org/abs/2112.04478) , <br> by *Chen Ju and
Tengda Han and
Kunhao Zheng and
Ya Zhang and
Weidi Xie* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L137-L155)<br> ```This paper upgrades CLIP image encoder to video encoder
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```CLIP_Video_2021```
## ResNet50

- [![](https://img.shields.io/badge/CoRR-2022-blue)](https://arxiv.org/abs/2205.03521) [**Good Visual Guidance Makes A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction**](https://arxiv.org/abs/2205.03521) , <br> by *Chen, Xiang, Zhang, Ningyu, Li, Lei, Yao, Yunzhi, Deng, Shumin, Tan, Chuanqi, Huang, Fei, Si, Luo and Chen, Huajun* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L81-L88)<br> ```This paper proposes visual prefix-guided fusion by concatenating object-level visual representation as the prefix of each self-attention layer in BERT
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```Chen_2022_HVPNeT```
- [![](https://img.shields.io/badge/CoRR-2021-blue)](https://arxiv.org/abs/2103.00020) [**Learning Transferable Visual Models From Natural Language Supervision**](https://arxiv.org/abs/2103.00020) , <br> by *Alec Radford and
Jong Wook Kim and
Chris Hallacy and
Aditya Ramesh and
Gabriel Goh and
Sandhini Agarwal and
Girish Sastry and
Amanda Askell and
Pamela Mishkin and
Jack Clark and
Gretchen Krueger and
Ilya Sutskever* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L114-L134)<br> ```This paper proposes to learn a multimodal embedding space by jointly training an image encoder and a text encoder
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```CLIP_2021```
- [![](https://img.shields.io/badge/CoRR-2021-blue)](https://arxiv.org/abs/2110.04544) [**CLIP-Adapter: Better Vision-Language Models with Feature Adapters**](https://arxiv.org/abs/2110.04544) , <br> by *Peng Gao and
Shijie Geng and
Renrui Zhang and
Teli Ma and
Rongyao Fang and
Yongfeng Zhang and
Hongsheng Li and
Yu Qiao* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L159-L179)<br> ```This paper is inspired by Adapter in order to transfer the knowledge of CLIP in order to implement Few-shot classification
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```CLIP-Adapter_2021```
## Faster R-CNN

- [![](https://img.shields.io/badge/CoRR-2022-blue)](https://arxiv.org/abs/2201.05078) [**CLIP-Event: Connecting Text and Images with Event Structures**](https://arxiv.org/abs/2201.05078) , <br> by *Manling Li and
Ruochen Xu and
Shuohang Wang and
Luowei Zhou and
Xudong Lin and
Chenguang Zhu and
Michael Zeng and
Heng Ji and
Shih{-}Fu Chang* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L93-L110)<br> ```This paper is inspired by CLIP using the Contrast Learning Framework, a method for connecting text and images using event structures
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```Li_Clip_Event_2022```
## Vision Transformer(ViT)

- [![](https://img.shields.io/badge/CoRR-2021-blue)](https://arxiv.org/abs/2103.00020) [**Learning Transferable Visual Models From Natural Language Supervision**](https://arxiv.org/abs/2103.00020) , <br> by *Alec Radford and
Jong Wook Kim and
Chris Hallacy and
Aditya Ramesh and
Gabriel Goh and
Sandhini Agarwal and
Girish Sastry and
Amanda Askell and
Pamela Mishkin and
Jack Clark and
Gretchen Krueger and
Ilya Sutskever* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L114-L134)<br> ```This paper proposes to learn a multimodal embedding space by jointly training an image encoder and a text encoder
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```CLIP_2021```
- [![](https://img.shields.io/badge/CoRR-2021-blue)](https://arxiv.org/abs/2112.04478) [**Prompting Visual-Language Models for Efficient Video Understanding**](https://arxiv.org/abs/2112.04478) , <br> by *Chen Ju and
Tengda Han and
Kunhao Zheng and
Ya Zhang and
Weidi Xie* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L137-L155)<br> ```This paper upgrades CLIP image encoder to video encoder
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```CLIP_Video_2021```
## Adapter

- [![](https://img.shields.io/badge/CoRR-2021-blue)](https://arxiv.org/abs/2110.04544) [**CLIP-Adapter: Better Vision-Language Models with Feature Adapters**](https://arxiv.org/abs/2110.04544) , <br> by *Peng Gao and
Shijie Geng and
Renrui Zhang and
Teli Ma and
Rongyao Fang and
Yongfeng Zhang and
Hongsheng Li and
Yu Qiao* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L159-L179)<br> ```This paper is inspired by Adapter in order to transfer the knowledge of CLIP in order to implement Few-shot classification
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```CLIP-Adapter_2021```