# Multi-modal Information Extraction Literature 
This repository is maintained by [Tongtong Wu](http://wutong8023.site/) and [Jingqi Kang](https://#####). 

The automation script of this repo is powered by [Auto-Bibfile](https://github.com/wutong8023/Auto-Bibfile.git).

You can directly use our bibtex.bib in overleaf with this [link](https://www.overleaf.com/read/gszxbvbkprfs).

This page categorizes the literature by the **Research Questions**.

## Outline 
- [![](https://img.shields.io/badge/Hyperlink-blue)](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/research_question\README.md#hyperlink)
- [![](https://img.shields.io/badge/Image_Text-10-blue)](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/research_question\README.md#image-text)
- [![](https://img.shields.io/badge/Video_Text-2-blue)](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/research_question\README.md#video-text)
## Hyperlink 
- [[Overview]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//README.md) -- [Homepage](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//README.md)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/./)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/./) -- [Summary](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/./)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/application)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/application) -- [Application](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/application)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/approach)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/approach) -- [Approach](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/approach)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/author)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/author) -- [Author](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/author)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/backbone_model)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/backbone_model) -- [Backbone Model](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/backbone_model)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/contribution)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/contribution) -- [Contribution](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/contribution)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/dataset)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/dataset) -- [Dataset](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/dataset)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/metrics)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/metrics) -- [Metrics](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/metrics)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/research_question)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/research_question) -- [Research Questions](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/research_question)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/setting)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/setting) -- [Setting](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/setting)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/supervision)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/supervision) -- [ Learning Paradigm](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/supervision)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/time)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/time) -- [Published Time](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/time)
- [[NLP]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4nlp/venue)  [[CV]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4cv/venue) -- [Published Venue](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//MMIE4all/venue)

## Image-Text

- [![](https://img.shields.io/badge/CoRR-2022-blue)](https://arxiv.org/abs/2205.03521) [**Good Visual Guidance Makes A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction**](https://arxiv.org/abs/2205.03521) , <br> by *Chen, Xiang, Zhang, Ningyu, Li, Lei, Yao, Yunzhi, Deng, Shumin, Tan, Chuanqi, Huang, Fei, Si, Luo and Chen, Huajun* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L81-L88)<br> ```This paper proposes visual prefix-guided fusion by concatenating object-level visual representation as the prefix of each self-attention layer in BERT
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```Chen_2022_HVPNeT```
- [![](https://img.shields.io/badge/CoRR-2022-blue)](https://arxiv.org/abs/2201.05078) [**CLIP-Event: Connecting Text and Images with Event Structures**](https://arxiv.org/abs/2201.05078) , <br> by *Manling Li and
Ruochen Xu and
Shuohang Wang and
Luowei Zhou and
Xudong Lin and
Chenguang Zhu and
Michael Zeng and
Heng Ji and
Shih{-}Fu Chang* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L93-L110)<br> ```This paper is inspired by CLIP using the Contrast Learning Framework, a method for connecting text and images using event structures
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```Li_Clip_Event_2022```
- [![](https://img.shields.io/badge/MM-2021-blue)](https://doi.org/10.1145/3474085.3476968) [**Multimodal Relation Extraction with Efficient Graph Alignment**](https://doi.org/10.1145/3474085.3476968) , <br> by *Zheng, Changmeng, Feng, Junhao, Fu, Ze, Cai, Yi, Li, Qing and Wang, Tao* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L71-L78)<br> ```This paper proposes a Multimodal Neural Network with Efficient Graph Alignment (MEGA) method for relation extraction in social media posts
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```Zheng_2021_MEGA```
- [![](https://img.shields.io/badge/CoRR-2021-blue)](https://arxiv.org/abs/2103.00020) [**Learning Transferable Visual Models From Natural Language Supervision**](https://arxiv.org/abs/2103.00020) , <br> by *Alec Radford and
Jong Wook Kim and
Chris Hallacy and
Aditya Ramesh and
Gabriel Goh and
Sandhini Agarwal and
Girish Sastry and
Amanda Askell and
Pamela Mishkin and
Jack Clark and
Gretchen Krueger and
Ilya Sutskever* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L114-L134)<br> ```This paper proposes to learn a multimodal embedding space by jointly training an image encoder and a text encoder
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```CLIP_2021```
- [![](https://img.shields.io/badge/CoRR-2021-blue)](https://arxiv.org/abs/2110.04544) [**CLIP-Adapter: Better Vision-Language Models with Feature Adapters**](https://arxiv.org/abs/2110.04544) , <br> by *Peng Gao and
Shijie Geng and
Renrui Zhang and
Teli Ma and
Rongyao Fang and
Yongfeng Zhang and
Hongsheng Li and
Yu Qiao* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L159-L179)<br> ```This paper is inspired by Adapter in order to transfer the knowledge of CLIP in order to implement Few-shot classification
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```CLIP-Adapter_2021```
- [![](https://img.shields.io/badge/CoRR-2021-blue)](https://arxiv.org/abs/2109.01134) [**Learning to Prompt for Vision-Language Models**](https://arxiv.org/abs/2109.01134) , <br> by *Kaiyang Zhou and
Jingkang Yang and
Chen Change Loy and
Ziwei Liu* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L182-L198)<br> ```This paper proposes to learn soft prompts represented by continuous context vectors, replacing the hand-designed Prompt in CLIP
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```CoOp_2021```
- [![](https://img.shields.io/badge/CoRR-2021-blue)](https://arxiv.org/abs/2112.01518) [**DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting**](https://arxiv.org/abs/2112.01518) , <br> by *Yongming Rao and
Wenliang Zhao and
Guangyi Chen and
Yansong Tang and
Zheng Zhu and
Guan Huang and
Jie Zhou and
Jiwen Lu* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L201-L221)<br> ```This paper proposes an instance-level prompt, where each data corresponds to a different prompt
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```DenseCLIP_2021```
- [![](https://img.shields.io/badge/ACL-2020-blue)](https://aclanthology.org/2020.acl-main.230) [**Cross-media Structured Common Space for Multimedia Event Extraction**](https://aclanthology.org/2020.acl-main.230) , <br> by *Li, Manling  and
Zareian, Alireza  and
Zeng, Qi  and
Whitehead, Spencer  and
Lu, Di  and
Ji, Heng  and
Chang, Shih-Fu* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L13-L27)<br> ```The first paper to define a multimodal event extraction task
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```li-etal-2020-cross```
- [![](https://img.shields.io/badge/AAAI-2020-blue)](https://ojs.aaai.org/index.php/AAAI/article/view/6437) [**Image Enhanced Event Detection in News Articles**](https://ojs.aaai.org/index.php/AAAI/article/view/6437) , <br> by *Tong, Meihan, Wang, Shuai, Cao, Yixin, Xu, Bin, Li, Juanzi, Hou, Lei and Chua, Tat-Seng* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L48-L56)<br> ```This paper proposes a multimodal fusion method with alternating dual attention
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```Tong_Wang_Cao_Xu_Li_Hou_Chua_2020```
- [![](https://img.shields.io/badge/MM-2017-blue)](https://doi.org/10.1145/3123266.3123294) [**Improving Event Extraction via Multimodal Integration**](https://doi.org/10.1145/3123266.3123294) , <br> by *Zhang, Tongtao, Whitehead, Spencer, Zhang, Hanwang, Li, Hongzhi, Ellis, Joseph, Huang, Lifu, Liu, Wei, Ji, Heng and Chang, Shih-Fu* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L2-L10)<br> ```The first paper to do multimodal event extraction
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```Zhang_VAD_2017```
## Video-Text

- [![](https://img.shields.io/badge/EMNLP-2021-blue)](https://aclanthology.org/2021.findings-emnlp.8) [**Joint Multimedia Event Extraction from Video and Article**](https://aclanthology.org/2021.findings-emnlp.8) , <br> by *Chen, Brian  and
Lin, Xudong  and
Thomas, Christopher  and
Li, Manling  and
Yoshida, Shoya  and
Chum, Lovish  and
Ji, Heng  and
Chang, Shih-Fu* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L30-L45)<br> ```This paper proposes a new task of video multimodal event extraction
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```chen-etal-2021-joint-multimedia-event```
- [![](https://img.shields.io/badge/CoRR-2021-blue)](https://arxiv.org/abs/2112.04478) [**Prompting Visual-Language Models for Efficient Video Understanding**](https://arxiv.org/abs/2112.04478) , <br> by *Chen Ju and
Tengda Han and
Kunhao Zheng and
Ya Zhang and
Weidi Xie* [[bib]](https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//./bibtex.bib#L137-L155)<br> ```This paper upgrades CLIP image encoder to video encoder
```</details><details><summary><img src=https://github.com/JingqiKang/Multi-modal-Information-Extraction/blob/main//scripts/svg/copy_icon.png height="20"></summary><pre>```CLIP_Video_2021```